---
title: "Kaggle_Project_1"
author: "Abhiram Mannam"
date: "2023-04-02"
output: 
  html_document:
    toc: true
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) #Importing the Library tidyverse.
library(DescTools)
```

### KAGGLE PROJECT HOUSE PREDICTION

The main aim of this project is to predict the house prices in Ames, Iowa with the given variables (predictors) and predict the SalePrice (Target Variable). 
As every home buyers need to buy their dream house but estimating the cost of the house is difficult. So, this model will estimate the price of the house using the variables (predictors) from the house. 
The benchmark for this model is to have r-squared above .75 using only 5 predictors (variables)


### Import and explore the datasets.

Importing the train and test data using read_csv.

```{r}
train <- read_csv("train.csv") # Get the Train set data.
test <- read_csv("test.csv") # Get the Test set data.
```

Exploring the dataframe structure for the train set and it's data types and names of every variable.

```{r}
str(train) # To get the structure of the train dataset. 
```

> Here we can see that the train dataset has 1460 rows with 80 columns (predictors) and one target variable (SalePrice). The dataset has num (numeric) and chr (character) datatypes. Some chr variables are factor variables which we need to convert it to factor before using them.

```{r}
str(test)
```

> Here we can see that the test dataset has 1459 rows with 80 columns (predictors). The dataset has num (numeric) and chr (character) datatypes. Some chr variables are factor variables which we need to convert it to factor before using them.

### Missing Values

To obtain the count of the missing values in the train dataset, we have created a function "count_missings" which inturn uses the is.na() function. There are 6965 missing values in the train dataset. Below are all the variables that have at least one NULL or NA values:

```{r}
count_missings <- function(x) sum(is.na(x))
train %>% 
  summarize_all(count_missings) 
```

> Here we have some of the missing or NA values for some of the variables for example Alley has 1369 and PoolQC has 1453 missing values. We can impute the missing values for the required variables of this model.

To clean the missing variables we impute the numeric variables with the mean and chr variables with mode or none.

```{r}

mean_LotFrontage <- mean(train$LotFrontage, na.rm = TRUE)
median_LotFrontage <- median(train$LotFrontage, na.rm = TRUE)

mean_MasVnrArea <- mean(train$LotFrontage, na.rm = TRUE)
median_MasVnrArea <- median(train$MasVnrArea, na.rm = TRUE)


train <- train %>%
  mutate(
    LotFrontage= ifelse (mean_LotFrontage > median_LotFrontage, median_LotFrontage,mean_LotFrontage),
         Alley = replace_na (Alley, replace = "None"),
         MasVnrType = replace_na (MasVnrType, replace = "None"),
         MasVnrArea = ifelse (mean_MasVnrArea > median_MasVnrArea, median_MasVnrArea,mean_MasVnrArea),
         BsmtQual = replace_na (BsmtQual, replace = "None"),
         BsmtCond = replace_na (BsmtCond, replace = "None"),
         BsmtExposure = replace_na (BsmtExposure, replace = "None"),
         BsmtFinType1 = replace_na (BsmtFinType1, replace = "None"),
         BsmtFinType2 = replace_na (BsmtFinType2, replace = "None"),
         Electrical = replace_na(Electrical,Mode(Electrical, na.rm = TRUE)),
         FireplaceQu = replace_na (FireplaceQu, replace = "None"),
         GarageType = replace_na (GarageType, replace = "None"),
         GarageYrBlt = replace_na (GarageYrBlt, replace = median(GarageYrBlt, na.rm = TRUE)),
         GarageFinish = replace_na (GarageFinish, replace = "None"),
         GarageQual = replace_na (GarageQual, replace = "None"),
         GarageCond = replace_na (GarageCond, replace = "None"),
         PoolQC = replace_na (PoolQC, replace = "None"),
         Fence = replace_na(Fence, replace = "None"),
         MiscFeature = replace_na(MiscFeature , replace = "None")
         )

```

And we can see that we have 0 missing values.

```{r}
train %>% 
  summarize_all(count_missings) 
sum(is.na(train))
```

```{r}
test %>% 
  summarize_all(count_missings)
```

To clean the missing variables we impute the numeric variables with the mean and chr variables with mode or none. 


```{r}

mean_LotFrontage <- mean(test$LotFrontage, na.rm = TRUE)
median_LotFrontage <- median(test$LotFrontage, na.rm = TRUE)

mean_MasVnrArea <- mean(test$MasVnrArea, na.rm = TRUE)
median_MasVnrArea <- median(test$MasVnrArea, na.rm = TRUE)

mean_BsmtFinSF2 <- mean(test$BsmtFinSF2, na.rm = TRUE)
median_BsmtFinSF2 <- median(test$BsmtFinSF2, na.rm = TRUE)

mean_BsmtUnfSF <- mean(test$BsmtUnfSF, na.rm = TRUE)
median_BsmtUnfSF <- median(test$BsmtUnfSF, na.rm = TRUE)

mean_TotalBsmtSF <- mean(test$TotalBsmtSF, na.rm = TRUE)
median_TotalBsmtSF <- median(test$TotalBsmtSF, na.rm = TRUE)

mean_BsmtFullBath <- mean(test$BsmtFullBath, na.rm = TRUE)
median_BsmtFullBath <- median(test$BsmtFullBath, na.rm = TRUE)

mean_BsmtHalfBath <- mean(test$BsmtHalfBath, na.rm = TRUE)
median_BsmtHalfBath <- median(test$BsmtHalfBath, na.rm = TRUE)

mean_GarageArea <- mean(test$GarageArea, na.rm = TRUE)
median_GarageArea <- median(test$GarageArea, na.rm = TRUE)



test <- test %>%
  mutate(MSZoning = replace_na(MSZoning,Mode(MSZoning, na.rm = TRUE)),
         LotFrontage= ifelse (mean_LotFrontage > median_LotFrontage, median_LotFrontage,mean_LotFrontage),
         Alley = replace_na (Alley, replace = "None"),
         Utilities = replace_na(Utilities,Mode(Utilities, na.rm = TRUE)),
         Exterior1st = replace_na(Exterior1st,Mode(Exterior1st, na.rm = TRUE)),
         Exterior2nd = replace_na(Exterior2nd,Mode(Exterior2nd, na.rm = TRUE)),
         MasVnrType = replace_na (MasVnrType, replace = "None"),
         MasVnrArea = ifelse (mean_MasVnrArea > median_MasVnrArea, median_MasVnrArea,mean_MasVnrArea),
         BsmtQual = replace_na (BsmtQual, replace = "None"),
         BsmtCond = replace_na (BsmtCond, replace = "None"),
         BsmtExposure = replace_na (BsmtExposure, replace = "None"),
         BsmtFinType1 = replace_na (BsmtFinType1, replace = "None"),
         BsmtFinSF1 = replace_na (BsmtFinSF1,0),
         BsmtFinType2 = replace_na (BsmtFinType2, replace = "None"),
         
         BsmtFinSF2 = ifelse (mean_BsmtFinSF2 > median_BsmtFinSF2, median_BsmtFinSF2,mean_BsmtFinSF2),
         
         BsmtUnfSF = ifelse (mean_BsmtUnfSF > median_BsmtUnfSF, median_BsmtUnfSF,mean_BsmtUnfSF),
         
         TotalBsmtSF = ifelse (mean_TotalBsmtSF > median_TotalBsmtSF, median_TotalBsmtSF,mean_TotalBsmtSF),
         
         BsmtFullBath = ifelse (mean_BsmtFullBath > median_BsmtFullBath, median_BsmtFullBath,mean_BsmtFullBath),
         
         BsmtHalfBath = ifelse (mean_BsmtHalfBath > median_BsmtHalfBath, median_BsmtHalfBath,mean_BsmtHalfBath),
         KitchenQual = replace_na(KitchenQual,Mode(KitchenQual, na.rm = TRUE)),
         Functional = replace_na(Functional,Mode(Functional, na.rm = TRUE)),
         FireplaceQu = replace_na (FireplaceQu, replace = "None"),
         GarageType = replace_na (GarageType, replace = "None"),
         GarageYrBlt = replace_na (GarageYrBlt, replace = median(GarageYrBlt, na.rm = TRUE)),
         GarageFinish = replace_na (GarageFinish, replace = "None"),
         GarageCars = replace_na(GarageCars,Mode(GarageCars, na.rm = TRUE)),
         GarageArea =  ifelse (mean_GarageArea > median_GarageArea, median_GarageArea,mean_GarageArea),
         GarageQual = replace_na (GarageQual, replace = "None"),
         GarageCond = replace_na (GarageCond, replace = "None"),
         PoolQC = replace_na (PoolQC, replace = "None"),
         Fence = replace_na(Fence, replace = "None"),
         MiscFeature = replace_na(MiscFeature, replace = "None"),
         SaleType = replace_na(SaleType,Mode(SaleType, na.rm = TRUE))
         )

```

```{r}
test %>% 
  summarize_all(count_missings)
sum (is.na(test))
```
We can see that we have 0 missing values.

```{r}
corr<-cor(train[sapply(train,is.numeric)])
tail(corr)
```

### Choosing the variables

As per my experience Neighborhood (where the house resides in the city) is the important variable for every house price. It increases from one Neighborhood to other Neighborhood. Neighborhood is chr variable so we can't use correlation to measure relation between those two variables. But, we can use linear regression model to model for the Neighborhhod and the target variable SalePrice.

```{r}
model1<- lm(SalePrice~LotFrontage*Neighborhood,data=train)
summary(model1)
```
We can see that R-squared value is 0.538. So, we can say that Neighborhood is the good predictor for the sales price.

We can see using the plot of between Neighborhood and Sales price.

```{r}
ggplot(train, aes((Neighborhood), SalePrice)) +
  geom_point() +
  labs(title = "SalePrice ~ Neighborhood")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

We can see that the plot that Neighborhood have an relationship with the sales price. That sales price depends on the Neighborhood.

Creating the summary table for the neighborhood using mean house price for every neighborhood.

```{r}
train %>%
  group_by(Neighborhood) %>%
  summarise(mean_average_sale_price = mean(SalePrice)) %>%
  arrange(desc(mean_average_sale_price)) %>%
  head(10)
```

Here from the summary table, we can see that NoRidge has his cost for houses, following by NridgHt and StoneBr.


Next is the OverallQual (Overall Material and Finish Quality) variable where every house is good quality or not. prices depend on the quality of the house. We can check the relation using correlation between OverallQual and SalesPrice.

```{r}
cor1<-cor(x=train$OverallQual, y=train$SalePrice)
cor1
```

We can see that correlation is 0.79 which shows OverallQual has positive correlation with the Sales Price.

Let's plot between OverallQual and SalesPrice 

```{r}
ggplot(train, aes(as.numeric(OverallQual), SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  labs(title = "SalePrice ~ OverallQual, with linear regression")
```

We can see that the plot has positive correlation of the OverallQual with the sales price.



Next is the GrLivArea (Above grade (ground) living area square feet) has positive effect on the price. High price has high GrLivArea. We can check this correlation.

```{r}
cor2<-cor(x=train$GrLivArea, y=train$SalePrice)
cor2
```

We can see that correlation is 0.70 which shows GrLivArea has positive correlation with the Sales Price.

```{r}
ggplot(train, aes(GrLivArea, SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  labs(title = "SalePrice ~ GrLivArea, with linear regression")
```

We can see from the plot that as sales Price increases with the increase in GrLivArea.



Next is the kitchenQual which shows what is the quality of the kitchen. It is the chr variable and it can be checked using the linear model whether it is a good variable to model or not.

```{r}
model2<- lm(SalePrice~KitchenQual,data=train)
summary(model2)
```

Here we can see that R- squared value is 0455 which shows that the KitchenQual is statistically significant and important variable for the model of sales price.

```{r}
train %>%
  group_by(KitchenQual) %>%
  summarise(mean_average_sale_price = mean(SalePrice)) %>%
  arrange(desc(mean_average_sale_price)) %>%
  head()
```

Here from the summary table, we can see that Ex(Excellent) has high price for houses followed by Gd (Good) and TA (Typical/Average) where Fa (Fair) type has low average sale_price.


Next is TotRmsAbvGrd which shows how many rooms above ground. It is a numeric variables and good relation with the price. It can be checked with the correlation.

```{r}
cor3<-cor(x=train$TotRmsAbvGrd, y=train$SalePrice)
cor3
```

We can see that correlation is 0.53 which shows TotRmsAbvGrd has positive correlation with the Sales 

```{r}
ggplot(train, aes(TotRmsAbvGrd, SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  labs(title = "SalePrice ~ TotRmsAbvGrd, with linear regression")
```

We can see that the TotRmsAbvGrd has positive relationship with the sales price.

So, from the above I have taken Neighborhood,TotRmsAbvGrd, OverallQual, KitchenQual, GrLivArea as the variables or predictors for the linear model.

### Cross-Validation

Cross-Validation is the method to divide the train data into two folds (mostly 70-30 split). One is train_fold and the other is validation_fold. We train the model on the train_fold and validate or test it on the validation_fold.

```{r}
set.seed(124)
# Randomly sample 70% of the rows
index <- sample(x = 1:nrow(train), size = nrow(train)*.7, replace = F)
head(index) # These are row numbers
```

```{r}
# Subset train using the index to create train_fold
train_fold <- train[index, ]
# Subset the remaining row to create validation fold.
validation_fold <- train[-index, ]
```

> Now we get train_fold with 70% data and validation_fold with 30% data. Fit model with the train_fold and test it on the validation_fold.

Let's model the data using the variables or predictors described above.

Here, modeling the target variable saleprice using predictore OverallQual, GrLivArea, Neighborhood, KitchenQual and TotRmsAbvGrd.
```{r}
model<-lm(log(SalePrice)~as.numeric(OverallQual)*factor(Neighborhood)+factor(KitchenQual)+log(`1stFlrSF`)*TotRmsAbvGrd+GrLivArea+GarageArea*GarageQual+MSSubClass+BsmtFullBath+log(LotArea)+Fireplaces+YearBuilt+Foundation+MasVnrArea+FullBath+Street*LandSlope+Functional+GarageFinish+Alley+CentralAir+Fence+BldgType,data=train_fold)
summary(model)
```


```{r}
# Building the model.

model1<-   lm(log(SalePrice) ~ Neighborhood * OverallQual + log(GrLivArea) + TotalBsmtSF * `1stFlrSF` + MasVnrArea  + KitchenQual + Alley + MSZoning + Utilities + LotArea + OverallCond + BldgType + GarageArea , data = train_fold) 

summary(model1) # Summary of the model.
```

```{r}
model<-  lm(log(SalePrice)~Neighborhood*OverallQual+log(GrLivArea) +TotRmsAbvGrd * `1stFlrSF`+factor(KitchenQual)+TotalBsmtSF+ GarageArea+log(LotArea)+Utilities+ Foundation+ MSZoning+BsmtQual+YearBuilt,data=train_fold)
summary(model)
```

```{r}
model<-  lm(SalePrice~HouseStyle,data=train_fold)
 
summary(model)
```

> Here we get Residual Standard Error as 34300 with R-squared as 0.8116. Which is greater than .75.

Here validating the model with the Validation fold and calculating the rmse and R2 for the validation fold using the model.

```{r}
predictions <- predict(model1, newdata = validation_fold) # Predicting the model with new data.
# Calculating the rmse.
rmse <- function(observed, predicted) sqrt(mean((observed - predicted)^2))

#Calculating the R2.
R2 <- function(observed, predicted){
  TSS <- sum((observed - mean(observed))^2)
  RSS <- sum((observed - predicted)^2)
  1- RSS/TSS
}

rmse(validation_fold$SalePrice, exp(predictions))
R2(validation_fold$SalePrice, exp(predictions))

```

> Here, the model has r-squared 0.7951 for the validation fold data with rmse is 37532.16

### Submission Model to the Kaggle 

Here we create the submission model for the full train data.

```{r}
submission_model <-  lm(log(SalePrice) ~ Neighborhood * OverallQual + log(GrLivArea) + TotalBsmtSF * `1stFlrSF` + MasVnrArea  + KitchenQual + Alley + MSZoning + Utilities + LotArea + OverallCond + BldgType + GarageArea,data=train)
summary(submission_model)
```

Here we predict the model for the test data.

```{r}
submission_predictions <- predict(submission_model, newdata = test)
```

Here we create new dataframe submission using test with id and submission_prediction columns.

```{r}
submission <- test %>% 
  select(Id) %>% 
  mutate(SalePrice = exp(submission_predictions))
```

Exploring the first 6 rows of submission dataframe.

```{r}
head(submission)
```

Output the submission dataframe to file to PC and uploading it into kaggle.

```{r}
write.csv(submission,"submission.csv")
```

### Residual Plot

Plotting the residual plot from the submission model. 

```{r}
train %>% 
  mutate(fitted = fitted(submission_model),
         residuals = SalePrice - fitted) %>% 
   ggplot(aes(fitted, residuals)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, col = "red") +
  labs(title = "residuals plot for the submission model")

```

> We can view the residual plot of the submission model and the plot seems that the residuals are random, that doesn't have any pattern and we can say the model is good fit for the given predictors.

```{r}
train %>% 
  mutate(fitted = fitted(submission_model),
         residuals = SalePrice - fitted) %>% 
  ggplot(aes(x=residuals))+
  geom_histogram() +
  labs(title = "histogram for the residuals of the submission model")
```

> We can also see that the plot of histogram of the residuals shows the histogram follows normal distribution which proves that the model is good fit to the data.


> So, to conclude we have taken the five predictors to predict the target variable SalePrice using the Neighborhood, OverallQual, KitchenQual, TotRmsAbvGrd, GrLivArea. We cleaned the train and test data and divided into train_fold and validation_fold with 70-30 split. We get aroung R2 .8 and rmse as 34300 for train_fold and R2 of .79 with rmse of 37532 with validation which shows that my model passes the threshold R2 of.75 and submit the model output with test data to kaggle.

> Submitting the submission file to the Kaggle will get the score of  0.17833.
